{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "optimizer got an empty parameter list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 162\u001b[0m\n\u001b[0;32m    160\u001b[0m model \u001b[38;5;241m=\u001b[39m ANNmodel()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    161\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m--> 162\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m    163\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mReduceLROnPlateau(optimizer, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    165\u001b[0m dataloaders \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m: train_loader,\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m: test_loader\n\u001b[0;32m    168\u001b[0m }\n",
      "File \u001b[1;32mc:\\Users\\ataheimer\\anaconda3\\Lib\\site-packages\\torch\\optim\\adamw.py:53\u001b[0m, in \u001b[0;36mAdamW.__init__\u001b[1;34m(self, params, lr, betas, eps, weight_decay, amsgrad, maximize, foreach, capturable, differentiable, fused)\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid weight_decay value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mweight_decay\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     41\u001b[0m defaults \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m     42\u001b[0m     lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[0;32m     43\u001b[0m     betas\u001b[38;5;241m=\u001b[39mbetas,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     51\u001b[0m     fused\u001b[38;5;241m=\u001b[39mfused,\n\u001b[0;32m     52\u001b[0m )\n\u001b[1;32m---> 53\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(params, defaults)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fused:\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m differentiable:\n",
      "File \u001b[1;32mc:\\Users\\ataheimer\\anaconda3\\Lib\\site-packages\\torch\\optim\\optimizer.py:279\u001b[0m, in \u001b[0;36mOptimizer.__init__\u001b[1;34m(self, params, defaults)\u001b[0m\n\u001b[0;32m    277\u001b[0m param_groups \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(params)\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(param_groups) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 279\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer got an empty parameter list\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(param_groups[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    281\u001b[0m     param_groups \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: param_groups}]\n",
      "\u001b[1;31mValueError\u001b[0m: optimizer got an empty parameter list"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, jaccard_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, RobustScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "df = pd.read_csv('cont_rlm_filled_capped.csv')\n",
    "df = df.drop('name', axis=1)\n",
    "\n",
    "\n",
    "X = df.drop('label', axis=1).values\n",
    "y = df['label'].values\n",
    "\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "Y = label_encoder.fit_transform(y)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "inputsize = X_train.shape[1]\n",
    "\n",
    "class ANNmodel(nn.Module):\n",
    "    def _init_(self):\n",
    "        super(ANNmodel, self)._init_()\n",
    "        self.ln1 = nn.Linear(inputsize, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.ln2 = nn.Linear(256, 512)\n",
    "        self.bn2 = nn.BatchNorm1d(512)\n",
    "        self.ln3 = nn.Linear(512, 1024)\n",
    "        self.bn3 = nn.BatchNorm1d(1024)\n",
    "        self.ln4 = nn.Linear(1024, 512)\n",
    "        self.bn4 = nn.BatchNorm1d(512)\n",
    "        self.ln5 = nn.Linear(512, 256)\n",
    "        self.bn5 = nn.BatchNorm1d(256)\n",
    "        self.ln6 = nn.Linear(256, 128)\n",
    "        self.bn6 = nn.BatchNorm1d(128)\n",
    "        self.ln7 = nn.Linear(128, 64)\n",
    "        self.bn7 = nn.BatchNorm1d(64)\n",
    "        self.ln8 = nn.Linear(64, 4)\n",
    "        self.dropout = nn.Dropout(0.6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.bn1(self.ln1(x)), negative_slope=0.01)\n",
    "        x = F.leaky_relu(self.bn2(self.ln2(x)), negative_slope=0.01)\n",
    "        x = self.dropout(x)\n",
    "        x = F.leaky_relu(self.bn3(self.ln3(x)), negative_slope=0.01)\n",
    "        x = F.leaky_relu(self.bn4(self.ln4(x)), negative_slope=0.01)\n",
    "        x = self.dropout(x)\n",
    "        x = F.leaky_relu(self.bn5(self.ln5(x)), negative_slope=0.01)\n",
    "        x = F.leaky_relu(self.bn6(self.ln6(x)), negative_slope=0.01)\n",
    "        x = self.dropout(x)\n",
    "        x = F.leaky_relu(self.bn7(self.ln7(x)), negative_slope=0.01)\n",
    "        x = torch.sigmoid(self.ln8(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "def save_best_model(model, path):\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "def train_model(model, dataloaders, criterion, optimizer, scheduler, num_epochs=100, patience=10):\n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in dataloaders['train']:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        epoch_loss = running_loss / len(dataloaders['train'].dataset)\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}, Loss: {epoch_loss:.4f}')\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in dataloaders['val']:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "        val_loss /= len(dataloaders['val'].dataset)\n",
    "        print(f'Validation Loss: {val_loss:.4f}')\n",
    "        scheduler.step(val_loss)\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            save_best_model(model, 'ANN_best_model_ManuelModel.pt')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "    end_time = time.time()\n",
    "    print(f'Training Time: {end_time - start_time:.2f} seconds')\n",
    "    model.load_state_dict(torch.load('ANN_best_model_ManuelModel.pt'))\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, dataloaders):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    y_scores = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloaders['val']:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "            y_scores.extend(F.softmax(outputs, dim=1).cpu().numpy())\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_scores = np.array(y_scores)\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    print(\"Accuracy Score:\", accuracy_score(y_true, y_pred))\n",
    "    print(\"Precision Score:\", precision_score(y_true, y_pred, average='weighted'))\n",
    "    print(\"Recall Score:\", recall_score(y_true, y_pred, average='weighted'))\n",
    "    print(\"F1 Score:\", f1_score(y_true, y_pred, average='weighted'))\n",
    "    print(\"Jaccard Score:\", jaccard_score(y_true, y_pred, average='weighted'))\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ANNmodel().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0001, weight_decay=0.01)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5, verbose=True)\n",
    "\n",
    "dataloaders = {\n",
    "    'train': train_loader,\n",
    "    'val': test_loader\n",
    "}\n",
    "\n",
    "model = train_model(model, dataloaders, criterion, optimizer, scheduler, num_epochs=500, patience=50)\n",
    "evaluate_model(model, dataloaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Model tanımının küçük bir düzeltme ile doğru çalışması için aşağıdaki şekilde olmalıdır:\n",
    "class ANNmodel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ANNmodel, self).__init__()\n",
    "        self.ln1 = nn.Linear(inputsize, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.ln2 = nn.Linear(256, 512)\n",
    "        self.bn2 = nn.BatchNorm1d(512)\n",
    "        self.ln3 = nn.Linear(512, 1024)\n",
    "        self.bn3 = nn.BatchNorm1d(1024)\n",
    "        self.ln4 = nn.Linear(1024, 512)\n",
    "        self.bn4 = nn.BatchNorm1d(512)\n",
    "        self.ln5 = nn.Linear(512, 256)\n",
    "        self.bn5 = nn.BatchNorm1d(256)\n",
    "        self.ln6 = nn.Linear(256, 128)\n",
    "        self.bn6 = nn.BatchNorm1d(128)\n",
    "        self.ln7 = nn.Linear(128, 64)\n",
    "        self.bn7 = nn.BatchNorm1d(64)\n",
    "        self.ln8 = nn.Linear(64, 4)\n",
    "        self.dropout = nn.Dropout(0.6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.bn1(self.ln1(x)), negative_slope=0.01)\n",
    "        x = F.leaky_relu(self.bn2(self.ln2(x)), negative_slope=0.01)\n",
    "        x = self.dropout(x)\n",
    "        x = F.leaky_relu(self.bn3(self.ln3(x)), negative_slope=0.01)\n",
    "        x = F.leaky_relu(self.bn4(self.ln4(x)), negative_slope=0.01)\n",
    "        x = self.dropout(x)\n",
    "        x = F.leaky_relu(self.bn5(self.ln5(x)), negative_slope=0.01)\n",
    "        x = F.leaky_relu(self.bn6(self.ln6(x)), negative_slope=0.01)\n",
    "        x = self.dropout(x)\n",
    "        x = F.leaky_relu(self.bn7(self.ln7(x)), negative_slope=0.01)\n",
    "        x = torch.sigmoid(self.ln8(x))\n",
    "        return x\n",
    "\n",
    "# Özellik önemini hesaplayan ve görselleştiren fonksiyon\n",
    "def plot_feature_importance(model, feature_names):\n",
    "    # Modelin ilk katmanındaki ağırlıkları al\n",
    "    weights = model.ln1.weight.cpu().detach().numpy()\n",
    "    # Ağırlıkların ortalamasını alarak önem derecelerini hesapla\n",
    "    importance = np.mean(np.abs(weights), axis=0)\n",
    "    \n",
    "    # Özellik önemlerini görselleştir\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(range(len(importance)), importance, tick_label=feature_names)\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Importance')\n",
    "    plt.title('Feature Importance')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n",
    "\n",
    "# Modeli eğitildikten sonra çağır\n",
    "feature_names = df.drop('label', axis=1).columns\n",
    "plot_feature_importance(model, feature_names)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
